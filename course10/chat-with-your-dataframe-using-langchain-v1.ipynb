{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5854b3da-34b8-4aa6-b9eb-afeacffa51d4",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e08912-3cf7-4a27-aef3-bf094de35e1f",
   "metadata": {},
   "source": [
    "# **Use Natural Language to Create Charts and Graphs**\n",
    "## **Build Your Own Data Visualization Agent**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f67101-a661-4309-9490-468e4b97ea45",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0cde0d-cfa0-419e-846e-ba319f967ea9",
   "metadata": {},
   "source": [
    "## Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155468a-8b21-454f-993b-dcd8ff092464",
   "metadata": {},
   "source": [
    "Imagine you are a data analyst or a data scientist of a marketing team at an e-commerce company. The company needs to understand customer purchasing behaviors over the last year to tailor their upcoming holiday campaigns. Traditionally, this would involve complex SQL queries, data wrangling in Python, and perhaps building visual dashboards to interpret the results including analyzing spreadsheets, creating charts, and maybe even some statistical analysisâ€”tasks that require considerable time and expertise.\n",
    "\n",
    "With the integration of Langchain and LLMs, you can simply ask, \"Show me a visualization of monthly sales trends by product category,\" or \"Generate a heatmap of customer activity by region.\" The system would use the `create_pandas_dataframe_agent` to process the CSV data, and then dynamically generate visualizations such as line graphs, bar charts, or heatmaps in response to these queries. This not only speeds up the data analysis process but also allows team members who may not be tech-savvy to engage directly with the data and make informed decisions quickly. This approach fosters a more collaborative environment and ensures that strategic decisions are backed by real-time data insights, visually represented for easy comprehension.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/V_7__WU_jHJ1lOpTeSLxTQ/chat%20with%20data.png\" width=\"50%\" alt=\"indexing\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb73fc5-7c06-4b3b-8d5f-3b4e9ad80654",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to seamlessly integrate data visualization into your conversational data analysis using Langchain and LLMs. Starting with CSV file data, you will use the `create_pandas_dataframe_agent` to build an interactive agent that not only understands and responds to your queries but also translates data responses into visual formats. You will explore how to dynamically generate charts, graphs, and heatmaps directly in response to natural language questions. This capability will enable you to visualize trends, compare figures, and spot patterns immediately, making your data analysis workflow both efficient and visually engaging. By the end of this project, you will have the skills to create a data conversational agent that acts as both analyst and visualizer, bringing data to life through dialogue.\n",
    "\n",
    "In this lab, you are going to use Llama 3.3 LLM hosted on the IBM watsonx.ai platform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274085e-9013-4f9f-962a-f9c59b745012",
   "metadata": {},
   "source": [
    "---------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3973b-eb9f-4fd0-a290-068639902bd1",
   "metadata": {},
   "source": [
    "## __Table of contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Overview\">Overview</a></li>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Data-set\">Data set</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Load-the-data-set\">Load the data set</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Load-LLM\">Load LLM</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Talk-to-your-data\">Talk to your data</a></li>\n",
    "            <li><a href=\"#Plot-your-data-with-natural-language\">Plot your data with natural language</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1---Relationship-between-parental-education-level-and-student-grades\">Exercise 1. Relationship between parental education level and student grades</a></li>\n",
    "    <li><a href=\"#Exercise-2---Impact-of-internet-access-at-home-on-grades\">Exercise 2. Impact of internet access at home on grades</a></li>\n",
    "    <li><a href=\"#Exercise-3---Explore-LLM's-code\">Exercise 3. Explore LLM's code</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a9a6f-6503-4d59-8776-7977c7d5d66e",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "\n",
    "After completing the project, you should be able to:\n",
    "\n",
    "- **Use LangChain with large language models**: Understand and apply the Langchain framework in conjunction with LLMs to interact with and analyze data stored in CSV files through natural language processing.\n",
    "- **Create conversational data agents**: Build a conversational agent that can understand and respond to natural language queries about data, enabling users to ask questions directly and receive immediate answers.\n",
    "- **Implement data visualization through dialogue**: Integrate data visualization tools within your conversational agent, allowing you to request and generate visual data representations such as graphs, charts, and heatmaps dynamically based on your queries.\n",
    "- **Enhance decision-making process**: Develop the capability to derive actionable insights from data via interactive dialogues and visual outputs, thereby improving the decision-making process and making data analysis accessible to non-technical stakeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3fd169-37b4-4ac7-8157-9ff0e7648df0",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a3455-0195-4d84-ab82-d708cea7540a",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c0c80-859f-4646-aee3-9d2c27c734a4",
   "metadata": {},
   "source": [
    "This project is based on Jupyter Notebook. If you're not familiar with it, here's a quick guide on how to run code within it:\n",
    "\n",
    "A Jupyter Notebook consists of cells. To execute a code cell, click on the cell that you want to run and click the 'Run' button, as shown in the picture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b0f62-aba5-4e10-87aa-41527b47fade",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IwbhiH3Wwv-VK-J4rioTAw/run.png\" width=\"50%\" alt=\"indexing\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820e090-1365-4707-b74d-9560dc6b558b",
   "metadata": {},
   "source": [
    "For this lab, you will be using the following libraries:\n",
    "\n",
    "*   [`ibm-watson-ai`](https://ibm.github.io/watson-machine-learning-sdk/index.html) for using LLMs from IBM's watsonx.ai.\n",
    "*   [`LangChain`, `langchain-ibm`, `langchain-experimental`](https://www.langchain.com/) for using its agent function to interact with data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5618d478-f409-46b4-8b9d-252dac54f406",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are __not__ preinstalled in the Skills Network Labs environment. __You must run the following cell__ to install them:\n",
    "\n",
    "**Note:** The version has been pinned here. It's recommended that you do this as well. Even if the library is updated in the future, the installed library could still support this lab work.\n",
    "\n",
    "This might take approximately 1-2 minutes. \n",
    "\n",
    "As you use `%%capture` to capture the installation, you won't see the output process. But after the installation completes, you will see a number beside the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b581dbb0-db46-431a-bdf8-4631ce956058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ibm-watsonx-ai==0.2.6\n",
      "  Downloading ibm_watsonx_ai-0.2.6-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting ibm-watson-machine-learning>=1.0.349 (from ibm-watsonx-ai==0.2.6)\n",
      "  Downloading ibm_watson_machine_learning-1.0.364-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: requests in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (2.32.4)\n",
      "Requirement already satisfied: urllib3 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (2.5.0)\n",
      "Collecting pandas<2.2.0,>=0.24.2 (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n",
      "  Downloading pandas-2.1.4.tar.gz (4.3 MB)\n",
      "     ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.3/4.3 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.3/4.3 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 0.5/4.3 MB 489.7 kB/s eta 0:00:08\n",
      "     ------- -------------------------------- 0.8/4.3 MB 667.5 kB/s eta 0:00:06\n",
      "     ------- -------------------------------- 0.8/4.3 MB 667.5 kB/s eta 0:00:06\n",
      "     ------- -------------------------------- 0.8/4.3 MB 667.5 kB/s eta 0:00:06\n",
      "     --------- ------------------------------ 1.0/4.3 MB 597.2 kB/s eta 0:00:06\n",
      "     --------- ------------------------------ 1.0/4.3 MB 597.2 kB/s eta 0:00:06\n",
      "     ------------ --------------------------- 1.3/4.3 MB 581.0 kB/s eta 0:00:06\n",
      "     ------------ --------------------------- 1.3/4.3 MB 581.0 kB/s eta 0:00:06\n",
      "     -------------- ------------------------- 1.6/4.3 MB 589.5 kB/s eta 0:00:05\n",
      "     ----------------- ---------------------- 1.8/4.3 MB 628.2 kB/s eta 0:00:04\n",
      "     ------------------- -------------------- 2.1/4.3 MB 668.2 kB/s eta 0:00:04\n",
      "     ------------------- -------------------- 2.1/4.3 MB 668.2 kB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 2.4/4.3 MB 680.3 kB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 2.4/4.3 MB 680.3 kB/s eta 0:00:03\n",
      "     ------------------------ --------------- 2.6/4.3 MB 686.1 kB/s eta 0:00:03\n",
      "     -------------------------- ------------- 2.9/4.3 MB 703.8 kB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 3.1/4.3 MB 724.2 kB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 3.1/4.3 MB 724.2 kB/s eta 0:00:02\n",
      "     ------------------------------- -------- 3.4/4.3 MB 730.5 kB/s eta 0:00:02\n",
      "     ------------------------------- -------- 3.4/4.3 MB 730.5 kB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 3.7/4.3 MB 714.1 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.9/4.3 MB 713.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.9/4.3 MB 713.8 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.3/4.3 MB 729.3 kB/s  0:00:05\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  â”‚ exit code: 2\n",
      "  â•°â”€> [98 lines of output]\n",
      "      + meson setup C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-1q73s4oc\\pandas_60fc477167154a0c9f1a0b59c57634f2 C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-1q73s4oc\\pandas_60fc477167154a0c9f1a0b59c57634f2\\.mesonpy-k78hto4v\\build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-1q73s4oc\\pandas_60fc477167154a0c9f1a0b59c57634f2\\.mesonpy-k78hto4v\\build\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.1\n",
      "      Source dir: C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-1q73s4oc\\pandas_60fc477167154a0c9f1a0b59c57634f2\n",
      "      Build dir: C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-1q73s4oc\\pandas_60fc477167154a0c9f1a0b59c57634f2\\.mesonpy-k78hto4v\\build\n",
      "      Build type: native build\n",
      "      Project name: pandas\n",
      "      Project version: 2.1.4\n",
      "      Activating VS 17.12.4\n",
      "      C compiler for the host machine: cl (msvc 19.42.34436 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.42.34436 for x64\")\n",
      "      C linker for the host machine: link link 14.42.34436.0\n",
      "      C++ compiler for the host machine: cl (msvc 19.42.34436 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.42.34436 for x64\")\n",
      "      C++ linker for the host machine: link link 14.42.34436.0\n",
      "      Cython compiler for the host machine: cython (cython 0.29.37)\n",
      "      Host machine cpu family: x86_64\n",
      "      Host machine cpu: x86_64\n",
      "      Program python found: YES (d:\\Course\\IBM DATA ANALYST PROFESSIONAL CERTIFICATE\\ibm_data_analyst_coding\\.venv\\Scripts\\python.exe)\n",
      "      Run-time dependency python found: YES 3.13\n",
      "      Build targets in project: 53\n",
      "      \n",
      "      pandas 2.1.4\n",
      "      \n",
      "        User defined options\n",
      "          Native files: C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-1q73s4oc\\pandas_60fc477167154a0c9f1a0b59c57634f2\\.mesonpy-k78hto4v\\build\\meson-python-native-file.ini\n",
      "          buildtype   : release\n",
      "          vsenv       : True\n",
      "          b_ndebug    : if-release\n",
      "          b_vscrt     : md\n",
      "      \n",
      "      Found ninja.EXE-1.13.0.git.kitware.jobserver-pipe-1 at C:\\Users\\akar\\AppData\\Local\\Temp\\pip-build-env-7nqaw08h\\normal\\Scripts\\ninja.EXE\n",
      "      \n",
      "      Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:\n",
      "      C:\\Users\\akar\\AppData\\Local\\Temp\\pip-build-env-7nqaw08h\\overlay\\Scripts\\meson compile -C .\n",
      "      + meson compile\n",
      "      Activating VS 17.12.4\n",
      "      INFO: automatically activated MSVC compiler environment\n",
      "      INFO: autodetecting backend as ninja\n",
      "      INFO: calculating backend command to run: C:\\Users\\akar\\AppData\\Local\\Temp\\pip-build-env-7nqaw08h\\normal\\Scripts\\ninja.EXE\n",
      "      [1/151] Generating pandas/_libs/algos_common_helper_pxi with a custom command\n",
      "      [2/151] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command\n",
      "      [3/151] Generating pandas/_libs/algos_take_helper_pxi with a custom command\n",
      "      [4/151] Generating pandas/_libs/index_class_helper_pxi with a custom command\n",
      "      [5/151] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command\n",
      "      [6/151] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command\n",
      "      [7/151] Generating pandas/_libs/intervaltree_helper_pxi with a custom command\n",
      "      [8/151] Generating pandas/_libs/sparse_op_helper_pxi with a custom command\n",
      "      [9/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/base.pyx\n",
      "      [10/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/ccalendar.pyx\n",
      "      [11/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/dtypes.pyx\n",
      "      [12/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/nattype.pyx\n",
      "      [13/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/np_datetime.pyx\n",
      "      [14/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/timezones.pyx\n",
      "      [15/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/fields.pyx\n",
      "      [16/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/conversion.pyx\n",
      "      [17/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/parsing.pyx\n",
      "      [18/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/strptime.pyx\n",
      "      [19/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/offsets.pyx\n",
      "      [20/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/arrays.pyx\n",
      "      [21/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/timestamps.pyx\n",
      "      [22/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/period.pyx\n",
      "      [23/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/timedeltas.pyx\n",
      "      [24/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/indexing.pyx\n",
      "      [25/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/tzconversion.pyx\n",
      "      [26/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslibs/vectorized.pyx\n",
      "      [27/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/ops_dispatch.pyx\n",
      "      [28/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/properties.pyx\n",
      "      [29/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/missing.pyx\n",
      "      [30/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/hashing.pyx\n",
      "      [31/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/internals.pyx\n",
      "      [32/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/byteswap.pyx\n",
      "      [33/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/ops.pyx\n",
      "      [34/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/parsers.pyx\n",
      "      [35/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/testing.pyx\n",
      "      [36/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/index.pyx\n",
      "      [37/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/sas.pyx\n",
      "      [38/151] Compiling C object pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj\n",
      "      \u001b[31mFAILED: [code=2] \u001b[0mpandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj\n",
      "      \"cl\" \"-Ipandas\\_libs\\tslibs\\base.cp313-win_amd64.pyd.p\" \"-Ipandas\\_libs\\tslibs\" \"-I..\\..\\pandas\\_libs\\tslibs\" \"-I..\\..\\..\\..\\pip-build-env-7nqaw08h\\overlay\\Lib\\site-packages\\numpy\\core\\include\" \"-I..\\..\\pandas\\_libs\\include\" \"-IC:\\Users\\akar\\AppData\\Local\\Programs\\Python\\Python313\\Include\" \"-DNDEBUG\" \"/MD\" \"/nologo\" \"/showIncludes\" \"/utf-8\" \"-w\" \"/O2\" \"/Gw\" \"-DNPY_NO_DEPRECATED_API=0\" \"-DNPY_TARGET_VERSION=NPY_1_21_API_VERSION\" \"/Fdpandas\\_libs\\tslibs\\base.cp313-win_amd64.pyd.p\\meson-generated_pandas__libs_tslibs_base.pyx.c.pdb\" /Fopandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj \"/c\" pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/base.pyx.c\n",
      "      pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/base.pyx.c(5397): error C2198: 'int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)': too few arguments for call\n",
      "      pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/base.pyx.c(5631): error C2198: 'int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)': too few arguments for call\n",
      "      [39/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/lib.pyx\n",
      "      [40/151] Compiling C object pandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.obj\n",
      "      \u001b[31mFAILED: [code=2] \u001b[0mpandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.obj\n",
      "      \"cl\" \"-Ipandas\\_libs\\tslibs\\ccalendar.cp313-win_amd64.pyd.p\" \"-Ipandas\\_libs\\tslibs\" \"-I..\\..\\pandas\\_libs\\tslibs\" \"-I..\\..\\..\\..\\pip-build-env-7nqaw08h\\overlay\\Lib\\site-packages\\numpy\\core\\include\" \"-I..\\..\\pandas\\_libs\\include\" \"-IC:\\Users\\akar\\AppData\\Local\\Programs\\Python\\Python313\\Include\" \"-DNDEBUG\" \"/MD\" \"/nologo\" \"/showIncludes\" \"/utf-8\" \"-w\" \"/O2\" \"/Gw\" \"-DNPY_NO_DEPRECATED_API=0\" \"-DNPY_TARGET_VERSION=NPY_1_21_API_VERSION\" \"/Fdpandas\\_libs\\tslibs\\ccalendar.cp313-win_amd64.pyd.p\\meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.pdb\" /Fopandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_ccalendar.pyx.c.obj \"/c\" pandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/ccalendar.pyx.c\n",
      "      pandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/ccalendar.pyx.c(7376): error C2198: 'int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)': too few arguments for call\n",
      "      pandas/_libs/tslibs/ccalendar.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/ccalendar.pyx.c(7686): error C2198: 'int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)': too few arguments for call\n",
      "      [41/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/reshape.pyx\n",
      "      [42/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/interval.pyx\n",
      "      [43/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/tslib.pyx\n",
      "      [44/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/writers.pyx\n",
      "      [45/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/window/indexers.pyx\n",
      "      [46/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/groupby.pyx\n",
      "      [47/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/window/aggregations.pyx\n",
      "      [48/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/sparse.pyx\n",
      "      [49/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/algos.pyx\n",
      "      [50/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/join.pyx\n",
      "      [51/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-1q73s4oc/pandas_60fc477167154a0c9f1a0b59c57634f2/pandas/_libs/hashtable.pyx\n",
      "      ninja: build stopped: subcommand failed.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Ã— Encountered error while generating package metadata.\n",
      "â•°â”€> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.1.16\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain==0.1.16)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain==0.1.16) (2.0.42)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.1.16)\n",
      "  Downloading aiohttp-3.12.15-cp313-cp313-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.16)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.1.16)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain==0.1.16)\n",
      "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain==0.1.16)\n",
      "  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.16)\n",
      "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.16)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2,>=1 (from langchain==0.1.16)\n",
      "  Using cached numpy-1.26.4-cp313-cp313-win_amd64.whl\n",
      "Collecting pydantic<3,>=1 (from langchain==0.1.16)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain==0.1.16) (2.32.4)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.16)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading multidict-6.6.4-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16)\n",
      "  Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl.metadata (76 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.1.16)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.42->langchain==0.1.16)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16)\n",
      "  Downloading orjson-3.11.3-cp313-cp313-win_amd64.whl.metadata (43 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: anyio in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.7.1)\n",
      "Requirement already satisfied: certifi in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (2025.7.14)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (3.10)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1->langchain==0.1.16)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1->langchain==0.1.16)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.16) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1->langchain==0.1.16)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.1.16) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from requests<3,>=2->langchain==0.1.16) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16) (3.2.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.16) (1.3.1)\n",
      "Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "   ---------------------------------------- 0.0/817.7 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 262.1/817.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 817.7/817.7 kB 2.8 MB/s  0:00:00\n",
      "Downloading aiohttp-3.12.15-cp313-cp313-win_amd64.whl (449 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.3/2.0 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 2.9 MB/s  0:00:00\n",
      "Downloading langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
      "Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.6.4-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Downloading orjson-3.11.3-cp313-cp313-win_amd64.whl (131 kB)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/2.0 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.6/2.0 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 1.6 MB/s  0:00:01\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.20.1-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.7.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading propcache-0.3.2-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, tenacity, PyYAML, pydantic-core, propcache, packaging, orjson, numpy, mypy-extensions, multidict, jsonpointer, h11, frozenlist, annotated-types, aiohappyeyeballs, yarl, typing-inspect, requests-toolbelt, pydantic, marshmallow, jsonpatch, httpcore, aiosignal, httpx, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "\n",
      "   ----------------------------------------  0/31 [typing-inspection]\n",
      "   - --------------------------------------  1/31 [tenacity]\n",
      "   - --------------------------------------  1/31 [tenacity]\n",
      "   -- -------------------------------------  2/31 [PyYAML]\n",
      "   -- -------------------------------------  2/31 [PyYAML]\n",
      "   -- -------------------------------------  2/31 [PyYAML]\n",
      "   -- -------------------------------------  2/31 [PyYAML]\n",
      "   --- ------------------------------------  3/31 [pydantic-core]\n",
      "   --- ------------------------------------  3/31 [pydantic-core]\n",
      "   ----- ----------------------------------  4/31 [propcache]\n",
      "  Attempting uninstall: packaging\n",
      "   ----- ----------------------------------  4/31 [propcache]\n",
      "    Found existing installation: packaging 25.0\n",
      "   ----- ----------------------------------  4/31 [propcache]\n",
      "    Uninstalling packaging-25.0:\n",
      "   ----- ----------------------------------  4/31 [propcache]\n",
      "      Successfully uninstalled packaging-25.0\n",
      "   ----- ----------------------------------  4/31 [propcache]\n",
      "   ------ ---------------------------------  5/31 [packaging]\n",
      "   ------ ---------------------------------  5/31 [packaging]\n",
      "   ------ ---------------------------------  5/31 [packaging]\n",
      "   ------ ---------------------------------  5/31 [packaging]\n",
      "   ------ ---------------------------------  5/31 [packaging]\n",
      "  Attempting uninstall: numpy\n",
      "   ------ ---------------------------------  5/31 [packaging]\n",
      "    Found existing installation: numpy 2.3.2\n",
      "   ------ ---------------------------------  5/31 [packaging]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "    Uninstalling numpy-2.3.2:\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "      Successfully uninstalled numpy-2.3.2\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   --------- ------------------------------  7/31 [numpy]\n",
      "   ----------- ----------------------------  9/31 [multidict]\n",
      "   ------------ --------------------------- 10/31 [jsonpointer]\n",
      "  Attempting uninstall: h11\n",
      "   ------------ --------------------------- 10/31 [jsonpointer]\n",
      "    Found existing installation: h11 0.12.0\n",
      "   ------------ --------------------------- 10/31 [jsonpointer]\n",
      "    Uninstalling h11-0.12.0:\n",
      "   ------------ --------------------------- 10/31 [jsonpointer]\n",
      "      Successfully uninstalled h11-0.12.0\n",
      "   ------------ --------------------------- 10/31 [jsonpointer]\n",
      "   -------------- ------------------------- 11/31 [h11]\n",
      "   -------------- ------------------------- 11/31 [h11]\n",
      "   -------------- ------------------------- 11/31 [h11]\n",
      "   ---------------- ----------------------- 13/31 [annotated-types]\n",
      "   ------------------ --------------------- 14/31 [aiohappyeyeballs]\n",
      "   ------------------- -------------------- 15/31 [yarl]\n",
      "   -------------------- ------------------- 16/31 [typing-inspect]\n",
      "   --------------------- ------------------ 17/31 [requests-toolbelt]\n",
      "   --------------------- ------------------ 17/31 [requests-toolbelt]\n",
      "   --------------------- ------------------ 17/31 [requests-toolbelt]\n",
      "   --------------------- ------------------ 17/31 [requests-toolbelt]\n",
      "   --------------------- ------------------ 17/31 [requests-toolbelt]\n",
      "   --------------------- ------------------ 17/31 [requests-toolbelt]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ----------------------- ---------------- 18/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [marshmallow]\n",
      "   ------------------------ --------------- 19/31 [marshmallow]\n",
      "   ------------------------ --------------- 19/31 [marshmallow]\n",
      "   ------------------------- -------------- 20/31 [jsonpatch]\n",
      "  Attempting uninstall: httpcore\n",
      "   ------------------------- -------------- 20/31 [jsonpatch]\n",
      "    Found existing installation: httpcore 0.13.7\n",
      "   ------------------------- -------------- 20/31 [jsonpatch]\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "    Uninstalling httpcore-0.13.7:\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "      Successfully uninstalled httpcore-0.13.7\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "  Attempting uninstall: httpx\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "    Found existing installation: httpx 0.20.0\n",
      "   --------------------------- ------------ 21/31 [httpcore]\n",
      "   ----------------------------- ---------- 23/31 [httpx]\n",
      "    Uninstalling httpx-0.20.0:\n",
      "   ----------------------------- ---------- 23/31 [httpx]\n",
      "      Successfully uninstalled httpx-0.20.0\n",
      "   ----------------------------- ---------- 23/31 [httpx]\n",
      "   ----------------------------- ---------- 23/31 [httpx]\n",
      "   ----------------------------- ---------- 23/31 [httpx]\n",
      "   ----------------------------- ---------- 23/31 [httpx]\n",
      "   ----------------------------- ---------- 23/31 [httpx]\n",
      "   ----------------------------- ---------- 23/31 [httpx]\n",
      "   ----------------------------- ---------- 23/31 [httpx]\n",
      "   ------------------------------ --------- 24/31 [dataclasses-json]\n",
      "   ------------------------------ --------- 24/31 [dataclasses-json]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   -------------------------------- ------- 25/31 [aiohttp]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   --------------------------------- ------ 26/31 [langsmith]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ---------------------------------- ----- 27/31 [langchain-core]\n",
      "   ------------------------------------ --- 28/31 [langchain-text-splitters]\n",
      "   ------------------------------------ --- 28/31 [langchain-text-splitters]\n",
      "   ------------------------------------ --- 28/31 [langchain-text-splitters]\n",
      "   ------------------------------------ --- 28/31 [langchain-text-splitters]\n",
      "   ------------------------------------ --- 28/31 [langchain-text-splitters]\n",
      "   ------------------------------------ --- 28/31 [langchain-text-splitters]\n",
      "   ------------------------------------ --- 28/31 [langchain-text-splitters]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   ------------------------------------- -- 29/31 [langchain-community]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   -------------------------------------- - 30/31 [langchain]\n",
      "   ---------------------------------------- 31/31 [langchain]\n",
      "\n",
      "Successfully installed PyYAML-6.0.2 aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 annotated-types-0.7.0 dataclasses-json-0.6.7 frozenlist-1.7.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.1.16 langchain-community-0.0.38 langchain-core-0.1.53 langchain-text-splitters-0.0.2 langsmith-0.1.147 marshmallow-3.26.1 multidict-6.6.4 mypy-extensions-1.1.0 numpy-1.26.4 orjson-3.11.3 packaging-23.2 propcache-0.3.2 pydantic-2.11.7 pydantic-core-2.33.2 requests-toolbelt-1.0.0 tenacity-8.5.0 typing-inspect-0.9.0 typing-inspection-0.4.1 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain-ibm==0.1.4\n",
      "  Downloading langchain_ibm-0.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting ibm-watsonx-ai<0.3.0,>=0.2.6 (from langchain-ibm==0.1.4)\n",
      "  Using cached ibm_watsonx_ai-0.2.6-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-ibm==0.1.4) (0.1.53)\n",
      "Collecting ibm-watson-machine-learning>=1.0.349 (from ibm-watsonx-ai<0.3.0,>=0.2.6->langchain-ibm==0.1.4)\n",
      "  Using cached ibm_watson_machine_learning-1.0.364-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (0.1.147)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (2.11.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (8.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (3.11.3)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (2.32.4)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (1.0.0)\n",
      "Requirement already satisfied: anyio in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (3.7.1)\n",
      "Requirement already satisfied: certifi in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-ibm==0.1.4) (2.5.0)\n",
      "Collecting pandas<2.2.0,>=0.24.2 (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai<0.3.0,>=0.2.6->langchain-ibm==0.1.4)\n",
      "  Using cached pandas-2.1.4.tar.gz (4.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): still running...\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  â”‚ exit code: 2\n",
      "  â•°â”€> [93 lines of output]\n",
      "      + meson setup C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-a0jg3zc0\\pandas_5b557d58e0bf44699637ad7973011594 C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-a0jg3zc0\\pandas_5b557d58e0bf44699637ad7973011594\\.mesonpy-py_wp5go\\build -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --vsenv --native-file=C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-a0jg3zc0\\pandas_5b557d58e0bf44699637ad7973011594\\.mesonpy-py_wp5go\\build\\meson-python-native-file.ini\n",
      "      The Meson build system\n",
      "      Version: 1.2.1\n",
      "      Source dir: C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-a0jg3zc0\\pandas_5b557d58e0bf44699637ad7973011594\n",
      "      Build dir: C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-a0jg3zc0\\pandas_5b557d58e0bf44699637ad7973011594\\.mesonpy-py_wp5go\\build\n",
      "      Build type: native build\n",
      "      Project name: pandas\n",
      "      Project version: 2.1.4\n",
      "      Activating VS 17.12.4\n",
      "      C compiler for the host machine: cl (msvc 19.42.34436 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.42.34436 for x64\")\n",
      "      C linker for the host machine: link link 14.42.34436.0\n",
      "      C++ compiler for the host machine: cl (msvc 19.42.34436 \"Microsoft (R) C/C++ Optimizing Compiler Version 19.42.34436 for x64\")\n",
      "      C++ linker for the host machine: link link 14.42.34436.0\n",
      "      Cython compiler for the host machine: cython (cython 0.29.37)\n",
      "      Host machine cpu family: x86_64\n",
      "      Host machine cpu: x86_64\n",
      "      Program python found: YES (d:\\Course\\IBM DATA ANALYST PROFESSIONAL CERTIFICATE\\ibm_data_analyst_coding\\.venv\\Scripts\\python.exe)\n",
      "      Run-time dependency python found: YES 3.13\n",
      "      Build targets in project: 53\n",
      "      \n",
      "      pandas 2.1.4\n",
      "      \n",
      "        User defined options\n",
      "          Native files: C:\\Users\\akar\\AppData\\Local\\Temp\\pip-install-a0jg3zc0\\pandas_5b557d58e0bf44699637ad7973011594\\.mesonpy-py_wp5go\\build\\meson-python-native-file.ini\n",
      "          buildtype   : release\n",
      "          vsenv       : True\n",
      "          b_ndebug    : if-release\n",
      "          b_vscrt     : md\n",
      "      \n",
      "      Found ninja.EXE-1.13.0.git.kitware.jobserver-pipe-1 at C:\\Users\\akar\\AppData\\Local\\Temp\\pip-build-env-nh7fmfsd\\normal\\Scripts\\ninja.EXE\n",
      "      \n",
      "      Visual Studio environment is needed to run Ninja. It is recommended to use Meson wrapper:\n",
      "      C:\\Users\\akar\\AppData\\Local\\Temp\\pip-build-env-nh7fmfsd\\overlay\\Scripts\\meson compile -C .\n",
      "      + meson compile\n",
      "      Activating VS 17.12.4\n",
      "      INFO: automatically activated MSVC compiler environment\n",
      "      INFO: autodetecting backend as ninja\n",
      "      INFO: calculating backend command to run: C:\\Users\\akar\\AppData\\Local\\Temp\\pip-build-env-nh7fmfsd\\normal\\Scripts\\ninja.EXE\n",
      "      [1/151] Generating pandas/_libs/algos_take_helper_pxi with a custom command\n",
      "      [2/151] Generating pandas/_libs/algos_common_helper_pxi with a custom command\n",
      "      [3/151] Generating pandas/_libs/hashtable_class_helper_pxi with a custom command\n",
      "      [4/151] Generating pandas/_libs/khash_primitive_helper_pxi with a custom command\n",
      "      [5/151] Generating pandas/_libs/hashtable_func_helper_pxi with a custom command\n",
      "      [6/151] Generating pandas/_libs/index_class_helper_pxi with a custom command\n",
      "      [7/151] Generating pandas/_libs/sparse_op_helper_pxi with a custom command\n",
      "      [8/151] Generating pandas/_libs/intervaltree_helper_pxi with a custom command\n",
      "      [9/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/base.pyx\n",
      "      [10/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/ccalendar.pyx\n",
      "      [11/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/dtypes.pyx\n",
      "      [12/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/np_datetime.pyx\n",
      "      [13/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/nattype.pyx\n",
      "      [14/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/arrays.pyx\n",
      "      [15/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/tzconversion.pyx\n",
      "      [16/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/timestamps.pyx\n",
      "      [17/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/timezones.pyx\n",
      "      [18/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/vectorized.pyx\n",
      "      [19/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/indexing.pyx\n",
      "      [20/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/conversion.pyx\n",
      "      [21/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/fields.pyx\n",
      "      [22/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/strptime.pyx\n",
      "      [23/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/hashing.pyx\n",
      "      [24/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/parsing.pyx\n",
      "      [25/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/offsets.pyx\n",
      "      [26/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/internals.pyx\n",
      "      [27/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/missing.pyx\n",
      "      [28/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/period.pyx\n",
      "      [29/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/ops_dispatch.pyx\n",
      "      [30/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslibs/timedeltas.pyx\n",
      "      [31/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/properties.pyx\n",
      "      [32/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/byteswap.pyx\n",
      "      [33/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/ops.pyx\n",
      "      [34/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/testing.pyx\n",
      "      [35/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/reshape.pyx\n",
      "      [36/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/sas.pyx\n",
      "      [37/151] Compiling C object pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj\n",
      "      \u001b[31mFAILED: [code=2] \u001b[0mpandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj\n",
      "      \"cl\" \"-Ipandas\\_libs\\tslibs\\base.cp313-win_amd64.pyd.p\" \"-Ipandas\\_libs\\tslibs\" \"-I..\\..\\pandas\\_libs\\tslibs\" \"-I..\\..\\..\\..\\pip-build-env-nh7fmfsd\\overlay\\Lib\\site-packages\\numpy\\core\\include\" \"-I..\\..\\pandas\\_libs\\include\" \"-IC:\\Users\\akar\\AppData\\Local\\Programs\\Python\\Python313\\Include\" \"-DNDEBUG\" \"/MD\" \"/nologo\" \"/showIncludes\" \"/utf-8\" \"-w\" \"/O2\" \"/Gw\" \"-DNPY_NO_DEPRECATED_API=0\" \"-DNPY_TARGET_VERSION=NPY_1_21_API_VERSION\" \"/Fdpandas\\_libs\\tslibs\\base.cp313-win_amd64.pyd.p\\meson-generated_pandas__libs_tslibs_base.pyx.c.pdb\" /Fopandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/meson-generated_pandas__libs_tslibs_base.pyx.c.obj \"/c\" pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/base.pyx.c\n",
      "      pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/base.pyx.c(5397): error C2198: 'int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)': too few arguments for call\n",
      "      pandas/_libs/tslibs/base.cp313-win_amd64.pyd.p/pandas/_libs/tslibs/base.pyx.c(5631): error C2198: 'int _PyLong_AsByteArray(PyLongObject *,unsigned char *,size_t,int,int,int)': too few arguments for call\n",
      "      [38/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/tslib.pyx\n",
      "      [39/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/writers.pyx\n",
      "      [40/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/window/indexers.pyx\n",
      "      [41/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/parsers.pyx\n",
      "      [42/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/window/aggregations.pyx\n",
      "      [43/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/index.pyx\n",
      "      [44/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/lib.pyx\n",
      "      [45/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/sparse.pyx\n",
      "      [46/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/interval.pyx\n",
      "      [47/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/algos.pyx\n",
      "      [48/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/hashtable.pyx\n",
      "      [49/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/groupby.pyx\n",
      "      [50/151] Compiling Cython source C:/Users/akar/AppData/Local/Temp/pip-install-a0jg3zc0/pandas_5b557d58e0bf44699637ad7973011594/pandas/_libs/join.pyx\n",
      "      ninja: build stopped: subcommand failed.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Ã— Encountered error while generating package metadata.\n",
      "â•°â”€> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-experimental==0.0.57\n",
      "  Downloading langchain_experimental-0.0.57-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.15 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-experimental==0.0.57) (0.1.16)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.41 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-experimental==0.0.57) (0.1.53)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2.0.42)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.0.38)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.0.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2.11.7)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2.32.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.0.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.41->langchain-experimental==0.0.57) (23.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.0.0)\n",
      "Requirement already satisfied: anyio in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.7.1)\n",
      "Requirement already satisfied: certifi in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\course\\ibm data analyst professional certificate\\ibm_data_analyst_coding\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain-experimental==0.0.57) (1.3.1)\n",
      "Downloading langchain_experimental-0.0.57-py3-none-any.whl (193 kB)\n",
      "Installing collected packages: langchain-experimental\n",
      "Successfully installed langchain-experimental-0.0.57\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install \"ibm-watsonx-ai==0.2.6\"\n",
    "%pip install \"langchain==0.1.16\" \n",
    "%pip install \"langchain-ibm==0.1.4\"\n",
    "%pip install \"langchain-experimental==0.0.57\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85847cac-c0ca-4a5b-98f6-4b39617aa348",
   "metadata": {},
   "source": [
    "After you installat the libraries, restart your kernel. You can do that by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/build-a-hotdog-not-hotdog-classifier-guided-project/images/Restarting_the_Kernel.png\" width=\"50%\" alt=\"Restart kernel\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0087b-4162-4837-8823-37925e06b984",
   "metadata": {},
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_It is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de2c3e9c-1c67-49d9-9287-bb61babf63f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ibm_watsonx_ai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m warnings.warn = warn\n\u001b[32m      6\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mibm_watsonx_ai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfoundation_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mibm_watsonx_ai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetanames\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GenTextParamsMetaNames \u001b[38;5;28;01mas\u001b[39;00m GenParams\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mibm_watson_machine_learning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfoundation_models\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextensions\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlangchain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WatsonxLLM\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ibm_watsonx_ai'"
     ]
    }
   ],
   "source": [
    "# You can use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7565da23-c5d6-4b29-b07e-a959a6a7f7c6",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df2c85-59c1-476a-a647-80b462a61c3f",
   "metadata": {},
   "source": [
    "In this lab, you will work on the Student Alcohol Consumption data set `student-mat.csv` by UCI Machine Learning as an example. For more information, see [Kaggle](https://www.kaggle.com/datasets/uciml/student-alcohol-consumption). It is based on data collected from two secondary schools in Portugal. The students included in the survey were in mathematics and Portuguese courses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2501f-756a-4c87-bae6-04b78e2168b3",
   "metadata": {},
   "source": [
    "The dataset you are using is for the mathematics course. The number of mathematics students involved in the collection was 395. The data collected in locations such as Gabriel Pereira and Mousinho da Silveira includes several pertinent values. Examples of such data are records of demographic information, grades, and alcohol consumption.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19284e52-b433-4287-b66c-d0f7eae47d24",
   "metadata": {},
   "source": [
    "| Field     | Description                                                                 |\n",
    "|-----------|-----------------------------------------------------------------------------|\n",
    "| school    | GP/MS for the student's school                                              |\n",
    "| sex       | M/F for gender                                                              |\n",
    "| age       | 15-22 for the student's age                                                 |\n",
    "| address   | U/R for urban or rural, respectively                                        |\n",
    "| famsize   | LE3/GT3 for less than or greater than three family members                  |\n",
    "| Pstatus   | T/A for living together or apart from parents, respectively                 |\n",
    "| Medu      | 0 (none) / 1 (primary-4th grade) / 2 (5th - 9th grade) / 3 (secondary) / 4 (higher) for mother's education |\n",
    "| Fedu      | 0 (none) / 1 (primary-4th grade) / 2 (5th - 9th grade) / 3 (secondary) / 4 (higher) for father's education |\n",
    "| Mjob      | 'teacher,' 'health' care related, civil 'services,' 'at_home' or 'other' for the student's mother's job |\n",
    "| Fjob      | 'teacher,' 'health' care related, civil 'services,' 'at_home' or 'other' for the student's father's job |\n",
    "| reason    | reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other') |\n",
    "| guardian  | mother/father/other as the student's guardian                               |\n",
    "| traveltime| 1 (<15mins) / 2 (15 - 30 mins) / 3 (30 mins - 1 hr) / 4 (>1hr) for a time from home to school |\n",
    "| studytime | 1 (<2hrs) / 2 (2 - 5hrs) / 3 (5 - 10hrs) / 4 (>10hrs) for weekly study time |\n",
    "| failures  | 1-3/4 for the number of class failures (if more than three, then record 4)  |\n",
    "| schoolsup | yes/no for extra educational support                                        |\n",
    "| famsup    | yes/no for family educational support                                       |\n",
    "| paid      | yes/no for extra paid classes for Math or Portuguese                        |\n",
    "| activities| yes/no for extra-curricular activities                                      |\n",
    "| nursery   | yes/no for whether attended nursery school                                  |\n",
    "| higher    | yes/no for the desire to continue studies                                   |\n",
    "| internet  | yes/no for internet access at home                                          |\n",
    "| romantic  | yes/no for relationship status                                              |\n",
    "| famrel    | 1-5 scale on quality of family relationships                                |\n",
    "| freetime  | 1-5 scale on how much free time after school             |\n",
    "| goout     | 1-5 scale on how much student goes out with friends      |\n",
    "| Dalc      | 1-5 scale on how much alcohol consumed on weekdays       |\n",
    "| Walc      | 1-5 scale on how much alcohol consumed on the weekend    |\n",
    "| health    | 1-5 scale on health condition                            |\n",
    "| absences  | 0-93 number of absences from school                      |\n",
    "| G1        | 0-20 for the first-period grade                          |\n",
    "| G2        | 0-20 for the second-period grade                         |\n",
    "| G3        | 0-20 for the final grade                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f46111-0bbc-4025-8547-aeadfb2cdbc7",
   "metadata": {},
   "source": [
    "### Load the data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e987c11-b6a6-441d-a2d2-ad78f8c1dc3a",
   "metadata": {},
   "source": [
    "Execute the code in the following cell to load your dataset. This code reads the CSV file into a pandas DataFrame, making the data accessible for processing in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58a6b4-724b-4da2-adc5-76b3f2e9c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/ZNoKMJ9rssJn-QbJ49kOzA/student-mat.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da895d-69b3-4e40-a505-74904039016c",
   "metadata": {},
   "source": [
    "Let's examine the first five rows of the dataset to get a glimpse of the data structure and its contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e901a8-241f-4898-bc7d-f9954ea73685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771b0085-5db0-43e9-835c-2b3d59770bbb",
   "metadata": {},
   "source": [
    "You can also review the detailed information for each column in the dataset, focusing on the presence of null values and the specific data types of each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b45cb89-ca7b-4991-9fbb-c536ade84665",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f594a-b9d5-4e0a-a97b-cbcf4336139a",
   "metadata": {},
   "source": [
    "## Load LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab908c8-4be3-476a-826f-d8c340a7b117",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to load the llama-3-3-70b LLM model from watsonx.ai. \n",
    "\n",
    "Additionally, you will configure the LLM to interact with data by integrating it with Langchain's `create_pandas_dataframe_agent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c75ece-bc7b-4338-9d26-4494a8b8aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store credential information\n",
    "credentials = {\n",
    "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\"\n",
    "}\n",
    "\n",
    "# Indicate the model you would like to initialize. In this case, Llama 3 70B.\n",
    "model_id    = 'meta-llama/llama-3-3-70b-instruct'\n",
    "\n",
    "# Initialize some watsonx.ai model parameters\n",
    "params = {\n",
    "        GenParams.MAX_NEW_TOKENS: 256, # The maximum number of tokens that the model can generate in a single run.\n",
    "        GenParams.TEMPERATURE: 0,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.\n",
    "    }\n",
    "project_id  = \"skills-network\" # <--- NOTE: specify \"skills-network\" as your project_id\n",
    "space_id    = None\n",
    "verify      = False\n",
    "\n",
    "# Launch a watsonx.ai model\n",
    "model = Model(\n",
    "    model_id=model_id, \n",
    "    credentials=credentials, \n",
    "    params=params, \n",
    "    project_id=project_id, \n",
    "    space_id=space_id, \n",
    "    verify=verify\n",
    ")\n",
    "\n",
    "# Integrate the watsonx.ai model with the langchain framework\n",
    "llm = WatsonxLLM(model = model)\n",
    "\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    llm,\n",
    "    df,\n",
    "    verbose=False,\n",
    "    return_intermediate_steps=True  # set return_intermediate_steps=True so that model could return code that it comes up with to generate the chart\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40620786-8995-4c04-b018-7f9cab4c6693",
   "metadata": {},
   "source": [
    "## API Disclaimer\n",
    "This lab uses LLMs provided by **Watsonx.ai**. This environment has been configured to allow LLM use without API keys so you can prompt them for **free (with limitations)**. With that in mind, if you wish to run this notebook **locally outside** of Skills Network's JupyterLab environment, you will have to **configure your own API keys**. Please note that using your own API keys means that you will incur personal charges.\n",
    "\n",
    "### Running Locally\n",
    "If you are running this lab locally, you will need to configure your own API keys. This lab uses the `WatsonxLLM` module from `langchain`. The local configuration is shown below with instructions. **Replace all instances** of both modules with the completed modules below throughout the lab. **DO NOT** run the cell below if you aren't running locally, it will causes errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2adbfc-63e3-4b1e-a166-04aa6fd6234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IGNORE IF YOU ARE NOT RUNNING LOCALLY\n",
    "\n",
    "credentials = {\n",
    "    \"url\"    : \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"api_key\": \"your api key here\"\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    model_id=model_id, \n",
    "    credentials=credentials, \n",
    "    params=params, \n",
    "    project_id=project_id, \n",
    "    space_id=space_id, \n",
    "    verify=verify\n",
    ")\n",
    "\n",
    "llm = WatsonxLLM(model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46df2b7-0663-4f4b-bdfc-3d106718a273",
   "metadata": {},
   "source": [
    "### Interact with your data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf011842-648f-44a8-bf0d-dcaa69dffb84",
   "metadata": {},
   "source": [
    "Let's start with a simple interaction.\n",
    "\n",
    "Ask LLM how many rows of data are in the CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d825b3d-1a87-42fa-9eb5-85666c106482",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\"how many rows of data are in this file?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57148347-bda9-4e3c-8bf3-a880d0e7a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3afdcb-1539-425f-8ef4-c628c02fe4a7",
   "metadata": {},
   "source": [
    "From the output above, the model reports that there are 395 rows of data in the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddb80f-c571-4988-933f-ee8df6772fd5",
   "metadata": {},
   "source": [
    "Let's verify this count using Python code to ensure accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce9f28-149f-482c-afde-07e294f89098",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dbdb24-094a-4163-a402-0acfa6885fdc",
   "metadata": {},
   "source": [
    "The row count matches and is correct! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fe847-5c8a-4db8-bde9-e2980a09a0db",
   "metadata": {},
   "source": [
    "Curious about the code the LLM generated and used to create this result?\n",
    "\n",
    "Run the code in the cell below to reveal the underlying commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56f2ae-7e1e-4b09-abd4-31bb92cc469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['intermediate_steps'][-1][0].tool_input.replace('; ', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8a4c0-4ef1-4353-8fa5-8e373413ae5f",
   "metadata": {},
   "source": [
    "Surprisingly, the LLM uses the same code as you do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd428b81-6eea-4076-9f61-8ab8ff7bd6df",
   "metadata": {},
   "source": [
    "Also, you could let LLM return some data that you are looking for based on the CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb707b2-0316-4fe1-85de-af1f2f562d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\"Give me all the data where student's age is over 18 years old.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784d290-b71b-4aad-949c-d30a33b982b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d63db3-4935-4abc-b040-bf5133388aba",
   "metadata": {},
   "source": [
    "Let's get the code LLM used for charting this plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80def40f-687f-434f-9630-ac78d29300c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response['intermediate_steps'][-1][0].tool_input.replace('; ', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8039a-cd47-4a98-9b32-7819ce5b63db",
   "metadata": {},
   "source": [
    "### Plot your data with natural language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650024c-8fa1-41d6-be7b-eafd52570319",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "Generating a first visual on the data set to know the total number of male and female students in the data set.\n",
    "\n",
    "You just need to tell the agent that \"Plot the gender count with bars.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0998300-40a1-4fae-a494-d734dc2e6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\"Plot the gender count with bars.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa907f1-5078-469f-bfd3-670d4825c55c",
   "metadata": {},
   "source": [
    "Let's see what code the LLM generated for ploting this chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3417eff-371f-4dcb-af3e-156144601154",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['intermediate_steps'][-1][0].tool_input.replace('; ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1998a16-01dc-4e6f-8d8d-bc6bccea22db",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "\n",
    "Generating a pie chart to display the average value of weekend alcohol for each gender in the dataset.\n",
    "\n",
    "You will use the prompt \"Generate a pie chart to display the average value of Walc for each gender.\"\n",
    "\n",
    "You may notice that the model generates two charts. The charts indicate the progressive improvement of the agent's code as it searches for the best way to answer your prompt, which improves the response to your query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf26cb-4b7a-49d4-8520-38f750a8c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\"Generate a pie chart to display average value of Walc for each Gender.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66595e4-a4a5-488d-afff-19aa40b93f52",
   "metadata": {},
   "source": [
    "Let's get the code LLM used for charting this plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2c2608-e17b-4f33-8b41-cc06c3897247",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['intermediate_steps'][-1][0].tool_input.replace('; ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aab328d-26b5-4f93-9c34-99ab387750e5",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "You can explore the impact of free time on grades based on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2589797-2283-4367-9ff0-ca6af7c37480",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\"Create box plots to analyze the relationship between 'freetime' (amount of free time) and 'G3' (final grade) across different levels of free time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63701308-963f-44e0-877e-67f982d5f285",
   "metadata": {},
   "source": [
    "Execute the code below to retrieve the Python script the LLM used for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040e754-f388-4dd7-82ad-9df83024e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['intermediate_steps'][-1][0].tool_input.replace('; ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1dfba-9de1-433d-a7ec-a57c4843b3cb",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "You can explore the effect of alcohol consumption on academic performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f170d471-2812-4fff-aa9a-69ff8fd9f1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\"Generate scatter plots to examine the correlation between 'Dalc' (daily alcohol consumption) and 'G3', and between 'Walc' (weekend alcohol consumption) and 'G3'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fb808-d3e2-474d-a055-01fa7d116dac",
   "metadata": {},
   "source": [
    "Execute the code below to retrieve the Python script the LLM used for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c2986-ba13-4633-a05a-6bdbac1d7e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['intermediate_steps'][-1][0].tool_input.replace('; ', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65f81d-a0cd-40b0-94bd-580d7c60d842",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d5df2-c1d9-4e64-a2a8-9910084c1b21",
   "metadata": {},
   "source": [
    "### Exercise 1 - Relationship between parental education level and student grades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44e1a3-05b2-4497-a453-4532393422b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc742c-7fd5-4219-bf5e-ab40f7c00660",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "response = agent.invoke(\"Plot scatter plots to show the relationship between 'Medu' (mother's education level) and 'G3' (final grade), and between 'Fedu' (father's education level) and 'G3'.\")\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187af0b-82e6-4289-b079-c9ebdb007d23",
   "metadata": {},
   "source": [
    "### Exercise 2 - Impact of internet access at home on grades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ac5be-6217-4494-b538-07d5bac4b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e1104-1ad1-4202-9efd-ce14b3fef7a2",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a solution</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "response = agent.invoke(\"Use bar plots to compare the average final grades ('G3') of students with internet access at home versus those without ('internet' column).\")\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6d11e-d5d7-4789-9c32-6a7686aeea20",
   "metadata": {},
   "source": [
    "### Exercise 3 - Explore LLM's code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f331502-103d-46e9-b0d7-4a47e365d0c5",
   "metadata": {},
   "source": [
    "Can you find what code the model used to generate the plot for exploring the relationship between absences and academic performance?\n",
    "\n",
    "You could run the corresponding code and from the response chain, you could see the code used from charting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f97e8cb-05fa-4493-a652-23a0d73b7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1aee5-d0f8-4d2f-b8a4-becacebfaa41",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a solution</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "response = agent.invoke(\"Plot a scatter plot showing the correlation between the number of absences ('absences') and final grades ('G3') of students.\")\n",
    "\n",
    "for i in range(len(response['intermediate_steps'])):\n",
    "    print(response['intermediate_steps'][i][0].tool_input.replace(';', '\\n'))\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58512a44-423d-4389-8e13-762c6aa1c29d",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c29eec-4af1-405c-a271-59296d2efeb4",
   "metadata": {},
   "source": [
    "[Kang Wang](https://author.skills.network/instructors/kang_wang)\n",
    "\n",
    "Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d4584-acfc-4f5f-b6f5-ac8e03c065d1",
   "metadata": {},
   "source": [
    "[Wojciech Fulmyk](https://author.skills.network/instructors/wojciech_fulmyk) <br>\n",
    "Wojciech \"Victor\" Fulmyk is a Data Scientist at IBM. He is also a PhD Candidate in Economics in the University of Calgary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc617aac-6dd9-4a75-b546-2ae073949e7b",
   "metadata": {},
   "source": [
    "## Other contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd76e9-b676-40c2-9ef5-4fad98a71529",
   "metadata": {},
   "source": [
    "[Ricky Shi](https://author.skills.network/instructors/ricky_shi) <br>\n",
    "Ricky Shi is a data scientist at the Ecosystems Skills Network at IBM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d96225-398b-4802-b97e-f95f06f41f2c",
   "metadata": {},
   "source": [
    "<!--## Change Log--!>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c529d-5f16-487b-8db4-0b40f64cf7d5",
   "metadata": {},
   "source": [
    "<!--|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-05-10|0.2|Kang Wang & Wojciech Fulmyk|Initial version created|\n",
    "|2024-02-23|0.1|Elio Di Nino|Update library documentation|--!>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22510b15-8028-4842-9316-c8532ccc4c30",
   "metadata": {},
   "source": [
    "## Copyright Â© IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "prev_pub_hash": "b6061b11be4dc0ff821d7aaa49a631d0eaf18b20de020b459cf737ac580f9be9"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
